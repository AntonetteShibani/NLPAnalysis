{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#https://tinyurl.com/ANLPTutorial2Part1\n",
        "## Go to \"File\" -> \"Save a Copy in Drive...\"\n",
        "\n",
        "This lets you create your own copy of the notebook in your Google drive, and any changes you make doesn't impact the shared notebook"
      ],
      "metadata": {
        "id": "Q1fgatIQGRDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text analysis using Python - Part 1"
      ],
      "metadata": {
        "id": "C-lUWaphEwK_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step is to install the required libraries using the pip command (if you don't have them), and import the modules from the libraries.\n",
        "\n"
      ],
      "metadata": {
        "id": "siLpslJoEXEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Enable plots to be displayed in the notebook\n",
        "%matplotlib inline\n",
        "\n",
        "!pip install seaborn\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "FFx6fp41EVoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpNlkY77RMcQ"
      },
      "source": [
        "## Mounting the drive\n",
        "\n",
        "In this notebook, I'm mounting the Google drive to read a csv file that is stored on my drive. You must allow access to your drive by signing in to your Google account."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBjL_EPjRKla"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the dataset from here: https://drive.google.com/file/d/1qAC9x-WMwzofyG8l1fFUoHwWhk3n61fp/view?usp=sharing\n",
        "\n",
        "Then, copy it to your Google drive folder which contains the notebook"
      ],
      "metadata": {
        "id": "aUdB-OV3rUkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# After executing the cell above, Drive files will be present in \"/content/drive/My Drive\". The below command lists the contents in the drive:\n",
        "!ls \"/content/drive/My Drive/Colab_Notebooks/ANLP\""
      ],
      "metadata": {
        "id": "WQmrB47_ugt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRuOvT-KR5hX"
      },
      "source": [
        "## Reading Data from a CSV File\n",
        "\n",
        "To read the data from the input csv file from my Google drive and store it as a Python dataframe, I use the read_csv() function from Pandas. You have to change the folder location to where the file is stored in your own Gdrive - mine is in this path: \n",
        "/content/drive/My Drive/Colab_Notebooks/ANLP/CNN_Articles_2021-2023.csv\n",
        "\n",
        "You can read about the different functions and their input parameters in the  documentation for the library:\n",
        "[Pandas Documentation](http://pandas.pydata.org/pandas-docs/stable/)\n",
        "\n",
        "**Note:** Comment code below if you are not importing from your Gdrive folder\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The input csv file is a subset of the data from https://github.com/hadasu/CNN_web_crawler\n",
        "newsdf = pd.read_csv('/content/drive/My Drive/Colab_Notebooks/ANLP/CNN_Articles_2021-2023.csv')"
      ],
      "metadata": {
        "id": "mNyLH32duQZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading input file from a url\n",
        "The alternative option is to read in the CSV from a web url (on github) and store it in a dataframe. This is a smaller dataset containing articles only from 2021 January to March.\n"
      ],
      "metadata": {
        "id": "67DM2moqmuSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://github.com/AntonetteShibani/NLPAnalysis/blob/main/CNN_Articles_2021.csv?raw=true'\n",
        "newsdf = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "ABIe5ntOo6lR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminary data inspection\n",
        "\n",
        "We usually try to get a a sense of the data first (particularly useful for large data sets where opening in other UI based tools is not easy)"
      ],
      "metadata": {
        "id": "qG-Q1XGgKh4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Print general information about a DataFrame including the index dtype and columns, non-null values and memory usage\n",
        "newsdf.info()"
      ],
      "metadata": {
        "id": "RD7R1zatKX4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newsdf.rename(columns={'Unnamed: 0': 'ID'}, inplace=True)"
      ],
      "metadata": {
        "id": "BHnSFho0v9y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate descriptive statistics that summarizes the central tendency, dispersion and shape of a dataset’s distribution\n",
        "newsdf.describe()"
      ],
      "metadata": {
        "id": "AE0pZ_0yLNRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHKzU83SzuWS"
      },
      "source": [
        "# Use the .head(n) function to look at the first 'n' rows of our news dataframe. The default n is 5, we are now changing it to view the first 10 rows\n",
        "newsdf.head(10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#A function similar to above, but provides a random sample of rows rather than the first few. \n",
        "newsdf.sample(5)"
      ],
      "metadata": {
        "id": "Va1qa7rtLq0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Count\n",
        "\n",
        "Word counts are simple but useful indicators for asking questions on the length of texts.\n",
        "\n",
        "To demonstrate usage, we see how the metrics are calculated for one sample sentence from the dataset. "
      ],
      "metadata": {
        "id": "ne2j1DzGJy7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = newsdf['headline'][2]\n",
        "print(s)\n",
        "\n",
        "#Splitting by whitespace characters and calculating the length. Note that punctuation marks are also counted as words\n",
        "len(s.split())"
      ],
      "metadata": {
        "id": "Coae8Z9hpWUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To make it easier to reuse in the future, we can create a function that returns word count\n",
        "def word_count(text):\n",
        "    wc = len(text.split())\n",
        "    return wc"
      ],
      "metadata": {
        "id": "QbJSpFNEOhjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now now we can apply the word_count function to our text variable to create a new variable with the number of words in the news article text."
      ],
      "metadata": {
        "id": "Qhe5E9UDaiTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newsdf['article_word_count'] = newsdf['text'].apply(word_count)"
      ],
      "metadata": {
        "id": "4VAJynJ4br3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use describe, hist, and scatter functions to provide some information on the length of articles in our dataset"
      ],
      "metadata": {
        "id": "RTgA9Z4_cxvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newsdf['article_word_count'].describe()"
      ],
      "metadata": {
        "id": "SOd1nHTebsAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newsdf['article_word_count'].hist(bins = 10)"
      ],
      "metadata": {
        "id": "oXvBS47ic_th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x = \"part_of\", \n",
        "            y = \"article_word_count\",\n",
        "            data =newsdf);"
      ],
      "metadata": {
        "id": "juhl86IXdHX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#I'm using a function that populates bar graph from a dataframe variable\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def wordBarGraphFunction(df,column,title):\n",
        "    topic_words = [ z.lower() for y in\n",
        "                       [ x.split() for x in df[column] if isinstance(x, str)]\n",
        "                       for z in y]\n",
        "    word_count_dict = dict(Counter(topic_words))\n",
        "    popular_words = sorted(word_count_dict, key = word_count_dict.get, reverse = True)\n",
        "    popular_words_nonstop = [w for w in popular_words if w not in stopwords.words(\"english\")]\n",
        "    plt.barh(range(50), [word_count_dict[w] for w in reversed(popular_words_nonstop[0:50])])\n",
        "    plt.yticks([x + 0.5 for x in range(50)], reversed(popular_words_nonstop[0:50]))\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "cmFwrYRqsQ44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "wordBarGraphFunction(newsdf,'headline',\"Most frequent words in news article headlines (Jan-Mar 2021)\")"
      ],
      "metadata": {
        "id": "4tgDzrXHsccM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can further explore the articles which are of the longest and shortest lengths"
      ],
      "metadata": {
        "id": "XBRZc7rHritQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#shortest\n",
        "newsdf.sort_values(by='article_word_count').head(10)"
      ],
      "metadata": {
        "id": "onZCq-mHrn5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#longest\n",
        "newsdf.sort_values(by='article_word_count', ascending=False).head(10)"
      ],
      "metadata": {
        "id": "qtDpdf1NsKoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can then examine the content of individual articles to gain additional insight as as needed."
      ],
      "metadata": {
        "id": "h2v79HwStMGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word frequencies\n",
        "\n",
        "Word frequencies (counting how often words occur) is a critical step in quantifying texts for many kinds of text analysis. There are inbuilt functions in Python that can compute words frequencies.\n",
        "\n",
        "Note that this analysis disregards the word order in the original sentence, taking a bag-of-words approach.\n"
      ],
      "metadata": {
        "id": "aEammaw6w257"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate frequencies to determine the most common word in the corpus"
      ],
      "metadata": {
        "id": "Ugg2B30rpXA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# converting series to string\n",
        "article_text = newsdf['text'].to_string()\n",
        "\n",
        "#create word tokens\n",
        "tokenized_words=word_tokenize(article_text)"
      ],
      "metadata": {
        "id": "4UrTROBE_-u9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_words=nltk.FreqDist(tokenized_words)\n",
        "all_words.plot(10);\n",
        "print(all_words.most_common(20))"
      ],
      "metadata": {
        "id": "_oCBNfxQ-1Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a word cloud to show most common words in the article text.\n",
        "\n",
        "Note: There are so many ways in which you can customise word clouds for display, check out the documentation and read related blogs posts to try different combinations."
      ],
      "metadata": {
        "id": "wIlWUjGRSlFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "wordcloud = WordCloud(max_words=100).generate(article_text)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PPMUWOkISiyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will notice that the most frequent terms are stopwords and punctuations, let's try recalculating frequencies after performing some basic cleaning."
      ],
      "metadata": {
        "id": "--Kr_8YULsjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# converting article text to lowercase as Python is case-sensitive\n",
        "article_text_lower = article_text.lower()\n",
        "\n",
        "#create word tokens\n",
        "tokenized_words=word_tokenize(article_text_lower)\n",
        "\n",
        "#Set up stop words for removal\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "#stopwords\n",
        "stop_words=stopwords.words(\"english\")\n",
        "print(stop_words)\n",
        "#Add custom stopwords to the list\n",
        "stop_words.extend([\"cnn\", \"'s\", \"a\", \"the\"])"
      ],
      "metadata": {
        "id": "jYscHfPDNPJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a new variable to store filtered tokens \n",
        "filtered_tokens=[]\n",
        "for w in tokenized_words:    \n",
        "    if w not in stop_words:\n",
        "         #add all filtered tokens excluding stopwords in this list below\n",
        "         filtered_tokens.append(w)\n",
        "\n",
        "import string\n",
        "# punctuations\n",
        "punctuations=list(string.punctuation)\n",
        "#Add custom punctuations to the list\n",
        "punctuations.append(\"...\")\n",
        "\n",
        "#Create another variable to store all clean tokens\n",
        "filtered_tokens_clean=[]\n",
        "for i in filtered_tokens:\n",
        "    if i not in punctuations:\n",
        "        filtered_tokens_clean.append(i)"
      ],
      "metadata": {
        "id": "xDdJoMlmL5Ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have cleaned the input text, let's calculate frequencies again to view the most common words."
      ],
      "metadata": {
        "id": "3I6rctiFQ0ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words=nltk.FreqDist(filtered_tokens_clean)\n",
        "all_words.plot(10);\n",
        "print(all_words.most_common(20))"
      ],
      "metadata": {
        "id": "2iimW5OLOM5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise: What are the insights from here? What do the key words indicate?"
      ],
      "metadata": {
        "id": "XleKubsVRCGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collocations\n",
        "It is also quite useful to find the most common words that co-occur. These two or three words that occur together are also known as BiGrams and TriGrams, but collocations are more meaningful than them. \n"
      ],
      "metadata": {
        "id": "1isvyVBxRPV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.collocations import *\n",
        "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
        "fourgram_measures = nltk.collocations.QuadgramAssocMeasures()\n",
        "finder = BigramCollocationFinder.from_words(filtered_tokens_clean)\n",
        "\n",
        "#Using PMI scores to quantify and rank the BiGrams\n",
        "finder.nbest(bigram_measures.pmi, 50)"
      ],
      "metadata": {
        "id": "Ik2pMa81UibF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word associations\n",
        "\n",
        "Let's see what the most associated words in our text are. You can repeat the same using trigrams."
      ],
      "metadata": {
        "id": "Nscor8dSoSsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import BigramAssocMeasures\n",
        "bigram_measures = BigramAssocMeasures()\n",
        "finder = BigramCollocationFinder.from_words(filtered_tokens_clean)\n",
        "\n",
        "finder.nbest(bigram_measures.likelihood_ratio, 20)"
      ],
      "metadata": {
        "id": "OhlFhMR2n1QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We can also find words associated with specific words of interest. Let's do this using trigrams in NLTK \n"
      ],
      "metadata": {
        "id": "uymda3lrkXJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
        "\n",
        "# Ngrams with 'city' as a member\n",
        "city_filter = lambda *w: 'city' not in w\n",
        "\n",
        "finder = TrigramCollocationFinder.from_words(filtered_tokens_clean)\n",
        "\n",
        "# only trigrams that appear 3+ times\n",
        "finder.apply_freq_filter(3)\n",
        "# only trigrams that contain 'city'\n",
        "finder.apply_ngram_filter(city_filter)\n",
        "\n",
        "# return the 10 n-grams with the highest PMI\n",
        "# print (finder.nbest(trigram_measures.likelihood_ratio, 10))\n",
        "for i in finder.score_ngrams(trigram_measures.likelihood_ratio):\n",
        "    print (i)"
      ],
      "metadata": {
        "id": "vm0iqutlknHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try it out: Visualise the key associations of words in a graph format\n",
        "\n",
        "https://lyonwj.com/blog/nlp-with-neo4j\n"
      ],
      "metadata": {
        "id": "SWeS7h-Q71Bn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concordances\n",
        "\n",
        "We can further look up the locations at which a given word occurs in the news articles using a concordance analysis.\n"
      ],
      "metadata": {
        "id": "uFIsTUUu5o1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.text import Text  \n",
        "textlist = Text(filtered_tokens_clean)\n",
        "print(textlist)\n",
        "textlist.concordance('city')\n",
        "textlist.concordance(\"city\", width=100, lines=10)"
      ],
      "metadata": {
        "id": "Q39mXqLh6d22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regular expressions\n",
        "\n",
        "A **regular expression** (or RE) is used to match strings of text such as particular characters, words, or patterns of characters. These come in quite handy for a number of operations in string manipulation. For instance, we can extract name from an email ID, Title from a name, subject code from a text description, or components of an address. \n",
        "\n",
        "There are commonly used wild card patterns in Python that helps us extract useful information from texts:\n",
        "^\n",
        "\n",
        "This wild card matches the characters at the beginning of a line.\n",
        "\n",
        "$\n",
        "\n",
        "This wild card matches the characters at the end of the line.\n",
        "\n",
        ".\n",
        "\n",
        "This wild card matches any character in the line.\n",
        "\n",
        "s\n",
        "\n",
        "This wild card is used to match space in a string.\n",
        "\n",
        "S\n",
        "\n",
        "This wild card matches non-whitespace characters.\n",
        "\n",
        "d\n",
        "\n",
        "This wild card matches one digit.\n",
        "\n",
        "*\n",
        "\n",
        "This wild card repeats any preceding character zero or more times. It matches the longest possible string.\n",
        "\n",
        "*?\n",
        "\n",
        "This wild card also repeats any preceding character/characters zero or more times. However, it matches the shortest string following the pattern.\n",
        "\n",
        "+\n",
        "\n",
        "This wild card repeats any preceding character one or more times. It matches the longest possible string following the pattern.\n",
        "\n",
        "+?\n",
        "\n",
        "This wild card repeats any preceding character one or more times. However, it matches the shortest possible string following the pattern.\n",
        "\n",
        "[aeiou]\n",
        "\n",
        "It matches any character from a set of given characters.\n",
        "\n",
        "[^XYZ]\n",
        "\n",
        "It matches any character not given in the set.\n",
        "\n",
        " [a-z0-9]\n",
        "\n",
        "It matches any character given in the a-z or 0-9.\n",
        "\n",
        "(\n",
        "\n",
        "This wild card represents the beginning of the string extraction.\n",
        "\n",
        ")\n",
        "\n",
        "This wild card represents the end of the string extraction.\n",
        "\n",
        "\n",
        "Read examples of applications here: https://code.tutsplus.com/tutorials/8-regular-expressions-you-should-know--net-6149, and more examples here: https://developers.google.com/edu/python/regular-expressions"
      ],
      "metadata": {
        "id": "d_BakwfDQ5zL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install regex\n",
        "import regex as re\n",
        "\n",
        "test_string = '''Guidelines for using ChatGPT in 36118: ANLP 2023\n",
        "To access ChatGPT, anyone can sign up for a free account at https://chat.openai.com/auth/login\n",
        "\n",
        "Key limitations of the technology\n",
        "It’s important to recognise that whilst AI tools enable us to do some amazing things, they have their limitations. Unlike search engines that provide you with options that match your query, ChatGPT gives you a definitive answer that may limit your own research abilities. Google has already entered this space, and there will be more to follow including the integration of GPT-3 in everyday tools such as Microsoft Teams. \n",
        "ChatGPT is a language model, not a knowledge base. \n",
        "One way to think about it is that it is a very sophisticated autocorrect. It has the capacity to generate text by anticipating what words should follow, creating cohesive and confident sounding sentences. However, it does not (and can not) validate the accuracy of the information it provides. These confident, but incorrect statements have been termed ‘hallucinations. In more problematic instances, it has even been shown to gaslight users. This subject requires you to do your own research towards topics of your interest, and ensure that information provided is correct.\n",
        "ChatGPT can’t recognise its sources. \n",
        "This means it can’t reference where the information is coming from. It has also been shown to make up false references and citations that do not exist in order to sound correct. As a language model, its objective is to present plausible sentences, not accurate information. This subject requires you to reference your research to ensure your understanding is founded on credible information.\n",
        "Limited data.\n",
        "ChatGPT has only been trained on data up until 2021. This means it does not have access to any up to date information, nor can it access new information as it is not connected to the internet. Whilst some argue that AI presents a paradigm shift for new creative outputs (WIRED), others argue that the limitations of AI data is a dangerous constraint on human imagination (Horning, 2022). This subject requires you to research and explore up to date trends and emerging signals, as well as challenging you to push your creativity.\n",
        "AI data has inherent (and often problematic) biases.\n",
        "An ongoing issue in the creation and implementation of AI tools is the inherent biases from training data. The reflection and reproduction of assumptions and biases has led to highly problematic results, like racist and sexist claims. This subject requires you to examine personal biases and critically reflect upon values in the futures you propose.\n",
        "Generative AI tools might have violated privacy laws.\n",
        "Large language models in GPT-3 and similar tools have obtained data from the web for training purposes, not all of which are openly available for public use. It’s been called a ‘privacy nightmare’ as the tool has over 300 billion words scraped from the internet: books, articles, websites and posts – including personal information obtained without consent. Other generative AI tools that generate images have also been under scrutiny as they face lawsuits for using Copyrighted works of art.\n",
        "ChatGPT is a proprietary tool\n",
        "ChatGPT is owned by OpenAI, which has control of the tool and your user data. The source code and the underlying models are not open sourced, meaning we do not know how the model actually works. The openly available version is a free research preview which does not guarantee access (You may need to pay for this in the future). Alternate open source models and tools are currently being developed - you may contribute to them!\n",
        "'''\n",
        "print(test_string)\n"
      ],
      "metadata": {
        "id": "pArnx7X5RfGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the example below, we extract all words that start with the letter 'C'"
      ],
      "metadata": {
        "id": "Ajz_B84pdPJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "startswithC = re.findall(r'(C\\w+)', test_string)\n",
        "\n",
        "for txt in startswithC:\n",
        "    print(txt)"
      ],
      "metadata": {
        "id": "kSB2PzQuTNIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Note how they are case-sensitive\n",
        "startswithC = re.findall(r'(c\\w+)', test_string)\n",
        "\n",
        "for txt in startswithC:\n",
        "    print(txt)"
      ],
      "metadata": {
        "id": "q7mVC9vniJvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise: Can you try creating one that captures lower case or upper case characters?"
      ],
      "metadata": {
        "id": "OEoAvuVpuKpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract URLS following a certain format\n",
        "print(re.search(\"(?P<url>https?://[^\\s]+)\", test_string).group(\"url\"))"
      ],
      "metadata": {
        "id": "Wbi0wdriedJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's write a function that can return matching texts and test it out with RegEx patterns."
      ],
      "metadata": {
        "id": "0ZJEcFrNdXpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_with_regex(regex, text):\n",
        "    matches = []\n",
        "    # find all matching patterns \n",
        "    for group in regex.findall(text):\n",
        "        matchingtext = ''.join(group)\n",
        "        matches.append(matchingtext)\n",
        "         \n",
        "    print(\"All matching texts: \")\n",
        "    print(matches)"
      ],
      "metadata": {
        "id": "dU8no7-GVtHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracting any integer\n",
        "pattern = re.compile(r'[0-9]')\n",
        "find_with_regex(pattern, test_string)"
      ],
      "metadata": {
        "id": "oouHlKhrW-Vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracting string with integers with at least 4 digits and at most 7 digits\n",
        "pattern = re.compile(r'\\d{4,7}(?!\\d)')\n",
        "find_with_regex(pattern, test_string)"
      ],
      "metadata": {
        "id": "iYJjzdqTXSLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: match() will only match if the string starts with the pattern. search() module will return the first occurrence that matches the specified pattern. findall() will iterate over all the lines of the file and will return all non-overlapping matches of pattern in a single step\n"
      ],
      "metadata": {
        "id": "T-XEFcwyYO5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise:** Try writing your own RegEx that can capture citations in text E.g. (Horning, 2022)"
      ],
      "metadata": {
        "id": "Q2zk9oNmhdP9"
      }
    }
  ]
}